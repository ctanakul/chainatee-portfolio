<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="../../docs-assets/ico/favicon.png">

    <title>CHAINATEE TANAKULRUNGSON</title>

    <!-- Bootstrap core CSS -->
    <link href="assets/css/bootstrap.css" rel="stylesheet">


    <!-- Custom styles for this template -->
    <link href="assets/css/main.css" rel="stylesheet">

    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="assets/js/hover.zoom.js"></script>
    <script src="assets/js/hover.zoom.conf.js"></script>

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <!-- Static navbar -->
    <div class="navbar navbar-inverse navbar-static-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">CHAINATEE TANAKULRUNGSON</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav navbar-right">
            <li><a href="work.html">Projects</a></li>
            <li><a href="about.html">About</a></li>
            <li><a href="resume.html">Resume</a></li>
            <!-- <li><a href="blog.html">Blog</a></li> -->
            <li><a href="contact.html">Contact</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>
	
	
	<!-- +++++ Projects Section +++++ -->
	
	<div class="container pt">
		<div class="row mt">
			<div class="col-lg-6 col-lg-offset-3 centered">
			  <h3>SAWYER CATCH'EM ALL</h3>
			  <hr>
			</div>
			<div class="col-lg-15">
			  <p>&nbsp;</p>
			  <p>&nbsp;</p>
                          <p><img class="img-responsive" src="assets/img/portfolio/work06_sawyer_catch_ball_diagram.png" alt=""></p>
			  <p>The goal of this project is to operate Sawyer, the manufacturing robot by Rethink Robotics, to catch a ball thrown at him. The hardware used are Sawyer and a stereo camera woth RGB and depth camera such as <a href="https://developer.microsoft.com/en-us/windows/kinect">Kinect</a> or <a href="https://www.asus.com/us/3D-Sensor/Xtion_PRO_LIVE/">Asus Xtion Pro</a> Code can be separated into 3 parts, the ball detection, the projectile trajectory estimator and the arm moving algorithm or inverse kinematics algorithm. All codes are coded in Python.</p>
			  <p>This project is an individual winter quarter project of MS in Robotics at Northwestern University.<!-- <a href="http://sdk.rethinkrobotics.com/wiki/API_Reference#Inverse_Kinematics_Solver_Service">IKservice.</a> --></p>



			  <!-- <p><b>Challenges</b></p> -->
			  <!-- <p> -->
			  <!--   <ul> -->
			  <!--     <li>Image processing in the aspect of the ball color detection</li> -->
			  <!--     <li>Pointcloud access and processing algorithm for real-time 3D position fetching</li> -->
			  <!--     <li>Inverse kinematics algorithm for Sawyer control</li> -->
			  <!--     <li>Everything must be done within 1 sec in each throw!!!</li> -->
			  <!--   </ul> -->
			  <!-- </p> -->

			  <p><b>Ball detection system</b></p>
			  <p>The ball detection system comprises of image processing algorithm for the ball's color and 3D position detection from RGB image and pointcloud.</p>
			  <p>The used stereo camera is <a href="https://www.asus.com/us/3D-Sensor/Xtion_PRO_LIVE/">Asus Xtion Pro Live</a> with RGB and depth sensor. The software for connecting and processing information from the hardware is <a href="http://wiki.ros.org/openni2_launch">openni2</a> package.</p>
			  <p>To get the 3D position of the ball, the image processing first detects the position of the ball in RGB image with <a href="https://github.com/ctanakul/sawyer-catching-ball/blob/master/src/detect_color_full_system.py#L238">color detection algorithm</a>. The image is smoothened with GaussianBlur and converted into HSV image to extract only a portion within specified HSV range. Noise is <a href="https://github.com/ctanakul/sawyer-catching-ball/blob/master/src/detect_color_full_system.py#L257">eroded</a> and the left portion is <a href="https://github.com/ctanakul/sawyer-catching-ball/blob/master/src/detect_color_full_system.py#L258">dilated</a>. The unmasked portion is further enclosed by a circle whose center pixel is outputted into <a href="http://docs.ros.org/api/image_geometry/html/python/#image_geometry.PinholeCameraModel.projectPixelTo3dRay">projectPixelTo3dRay</a> to get the unit vector pointing to that pixel then it is scaled up by the depth of that pixel from <a href="https://github.com/ctanakul/sawyer-catching-ball/blob/master/src/detect_color_full_system.py#L192">depth image</a>, therefore the 3D position of the ball can be found.
Users can adjust the HSV range and the size of eroding and dilating kernel with <a href="https://github.com/ctanakul/sawyer-catching-ball/blob/master/src/detect_color_full_system.py#L57">trackbars</a> provided with the algorithm.</p>

			  <p><b>Projectile calculating algorithm</b></p>
			  <p>The <a href="https://github.com/ctanakul/sawyer-catching-ball/blob/master/src/sawyer_catch_ball_calc.py#L29">projectile calculating algorithm</a> estimates the future position of the ball from the first few positions in 3D at the beginning of the throw. Every two positions can be used to find initial velocity and launch angle in horizontal and vertical plane to estimate the future position of the ball.</p>
			  <p>In order to check the precision of the system without running real robot, the predicting position is <a href="https://github.com/ctanakul/sawyer-catching-ball/blob/master/src/ik_controller_full_system.py#L207">simulated as a frame</a> in RVIZ to compare between the final position of the ball in physical and simulated world. </p>

			  <p><b>Inverse Kinematics Controller</b></p>
			  <p>The frequency of published pointcloud is adjusted to be at 30 Hz. The assumed number of 3D positions needed for future position estimation is 5 positions which will take about 15 milliseconds. Since the estimated time used in a throw is about 1 second, the time for the arm to move to catch the ball is about 85 milliseconds. For simplicity, the planar area for moving hand is specified. This area can be found by moving the arm and timing the total time used.<p>
			  <p>Before every move, the hand will be moved to home position by sending the dictionary containing names and joint positions as a parameter in <a href="http://api.rethinkrobotics.com/baxter_interface/html/baxter_interface.limb.Limb-class.html">move_to_joint_positions</a> function of class Limb. The
<a href="http://web.cse.ohio-state.edu/~parent/classes/694A/Lectures/Material/IKsurvey.pdf">damped least square inverse kinematics</a> is used to find joint velocity for each joint in robot arm while
moving. The damping constant delays time in finding joint solution to avoid singularities. However,high damping constant will slow the convergence time a lot. Joint angular velocities are then
published under the topic of robot/limb/right/joint_command of type<a href="http://api.rethinkrobotics.com/baxter_core_msgs/html/msg/JointCommand.html">intera_core_msgs/JointCommand</a>.</p>
			  <p>The approximate area that the hand can be moved within 85 milliseconds is 30 x 30 cm square on the right side and a semicircle with 30 cm radius on the left side of the home position.</p>

			  <p><b>Present development</b></p>
			  <p>The present challenge is to get the correct 3D position of the ball from pointcloud. Normally, pointcloud can return the 3D position quite accurate. However, for moving object such as a thrown ball, the stereo-correspondence may fail which will return depth of NaN (Non-applicable Number). With NaN depth, the 3D position of the ball cannot be found which lead to the loss of data for trajectory prediction.</p>
			  <p>However, with the assumption that some pixels around such problematic pixels may not fail in returning depth, the author finds depth of every 30 x 30 pixels around the center and find the average depth of all. Outlier depths which may be returned from those points not being part of a ball such as wall will be filtered out. This process can return depth for every pixel specified as the center of the ball.</p>
			  <p>To check the accuracy and analyze the data, the coordinate of pixels specified as center of the ball and the 3D position of the ball are <a href="https://github.com/ctanakul/sawyer-catching-ball/blob/master/src/detect_color_full_system.py#L134">plotted through time</a>.</p>


			  <!-- <p><b>Solutions</b></p> -->
			  <!-- <p> -->
			  <!--   <ul> -->
			  <!--     <li>Use functions of OpenCV such as GaussianBlur and InRange in HSV spectrum to get the position of the ball</li> -->
			  <!--     <li>Access pointcloud via <a href="https://www.asus.com/us/3D-Sensor/Xtion_PRO_LIVE/">Asus Xtion Pro Live</a> with <a href="http://wiki.ros.org/openni2_launch">openni2</a>, functions of <a href="http://docs.ros.org/api/image_geometry/html/python/#image_geometry.PinholeCameraModel.projectPixelTo3dRay">image_geometry</a> and noise filtering algorithm</li> -->
			  <!--     <li><a href="http://web.cse.ohio-state.edu/~parent/classes/694A/Lectures/Material/IKsurvey.pdf">Use damped least square inverse kinematics</a> to avoid singularities</li> -->
			  <!--   </ul> -->
			  <!-- </p> -->
                          <p><img class="img-responsive" src="assets/img/portfolio/work06_sawyer_catch_ball.gif" alt=""></p>


			  <p><b>Let's see more</b></p>
			  <p><a href="https://github.com/ctanakul/sawyer-catching-ball">Github</a><!-- <a href="https://youtu.be/ngZCMMJjaLA">Youtube</a>, --> <!-- <a href="https://vimeo.com/195121510">Vimeo</a></p> -->


			</div>
		</div>
		<!-- <div class="row mt centered">	 -->
		<!--   <p><iframe width="840" height="472.5" src="https://www.youtube.com/embed/ngZCMMJjaLA" frameborder="0" allowfullscreen></iframe></p> -->
		<!-- </div> -->
		<div class="row mt">
		  <p><b>Acknowledgements</b></p>
		  <p>
		    <ul>
		      <li>Dr.Jarvis A. Schultz, the associate director of Ms in Robotics, Northwestern University, Fall 2016</li>
		    </ul>
		  <p>
		</div>

	</div><!-- /container -->
	
	
	<!-- +++++ Footer Section +++++ -->
	
	<div id="footer">
		<div class="container">
			<div class="row">
				<div class="col-lg-4">
					<h4>Address</h4>
				  <p>
				    2145 Sheridan Rd., Evanston, IL, 60201<br/>
				    +1 773 865 0534, <br/>
				    <!-- Madrid, Spain. -->
				  </p>
				</div><!-- /col-lg-4 -->
				
				<div class="col-lg-4">
					<h4>Links</h4>
					<p>
						<a href="https://github.com/ctanakul">Github</a><br/>
						<a href="https://www.linkedin.com/in/chainatee-tanakulrungson">Linkedin</a><br/>
						<!-- <a href="#">Facebook</a> -->
					</p>
				</div><!-- /col-lg-4 -->
				
				<!-- <div class="col-lg-4"> -->
				<!-- 	<h4>About Stanley</h4> -->
				<!-- 	<p>This cute theme was created to showcase your work in a simple way. Use it wisely.</p> -->
				<!-- </div><\!-- /col-lg-4 -\-> -->
			
			</div>
		
		</div>
	</div>
	

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="assets/js/bootstrap.min.js"></script>
  </body>
</html>
